{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9975ce",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6fe8d",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the Ridge Regression algorithm.\n",
    "\n",
    "First, let’s introduce a standard regression dataset. We will use the housing dataset.\n",
    "\n",
    "The housing dataset is a standard machine learning dataset comprising 506 rows of data with 13 numerical input\n",
    "variables and a numerical target variable.\n",
    "\n",
    "Using a test harness of repeated stratified 10-fold cross-validation with three repeats, a naive model can achieve\n",
    "a mean absolute error (MAE) of about 6.6. A top-performing model can achieve a MAE on this same test harness of\n",
    "about 1.9. This provides the bounds of expected performance on this dataset.\n",
    "\n",
    "The dataset involves predicting the house price given details of the house’s suburb in the American city of Boston.\n",
    "\n",
    "Housing Dataset (housing.csv)\n",
    "Housing Description (housing.names)\n",
    "No need to download the dataset; we will download it automatically as part of our worked examples.\n",
    "\n",
    "The example below downloads and loads the dataset as a Pandas DataFrame and summarizes the shape of the dataset\n",
    "and the first five rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e5f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "# load and summarize the housing dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# summarize shape\n",
    "print(dataframe.shape)\n",
    "# summarize first few lines\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0be47",
   "metadata": {},
   "source": [
    "Running the example confirms the 506 rows of data and 13 input variables and a single numeric target variable (14 in total). We can also see that all input variables are numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a81d8b",
   "metadata": {},
   "source": [
    "The scikit-learn Python machine learning library provides an implementation of the Ridge Regression algorithm via\n",
    "the Ridge class.\n",
    "\n",
    "Confusingly, the lambda term can be configured via the “alpha” argument when defining the class. The default values 1.0 or a full penalty.\n",
    "\n",
    "We can evaluate the Ridge Regression model on the housing dataset using repeated 10-fold cross-validation and report the average mean absolute error (MAE) on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d98c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 3.382 (0.519)\n"
     ]
    }
   ],
   "source": [
    "# evaluate an ridge regression model on the dataset\n",
    "\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model\n",
    "model = Ridge(alpha=1.0)\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6e989",
   "metadata": {},
   "source": [
    "Running the example evaluates the Ridge Regression algorithm on the housing dataset and reports the average MAE across the three repeats of 10-fold cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.\n",
    "\n",
    "In this case, we can see that the model achieved a MAE of about 3.382."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433d777",
   "metadata": {},
   "source": [
    "We may decide to use the Ridge Regression as our final model and make predictions on new data.\n",
    "\n",
    "This can be achieved by fitting the model on all available data and calling the predict() function, passing in a \n",
    "new row of data.\n",
    "\n",
    "We can demonstrate this with a complete example listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177335b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 30.253\n"
     ]
    }
   ],
   "source": [
    "# make a prediction with a ridge regression model on the dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model\n",
    "model = Ridge(alpha=1.0)\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# define new data\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "# make a prediction\n",
    "yhat = model.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cbf57",
   "metadata": {},
   "source": [
    "# Tuning Ridge Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850606a",
   "metadata": {},
   "source": [
    "How do we know that the default hyperparameters of alpha=1.0 is appropriate for our dataset?\n",
    "\n",
    "We don’t.\n",
    "\n",
    "Instead, it is good practice to test a suite of different configurations and discover what works best for our dataset.\n",
    "\n",
    "One approach would be to grid search alpha values from perhaps 1e-5 to 100 on a log scale and discover what works best for a dataset. Another approach would be to test values between 0.0 and 1.0 with a grid separation of 0.01. We will try the latter in this case.\n",
    "\n",
    "The example below demonstrates this using the GridSearchCV class with a grid of values we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search hyperparameters for ridge regression\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model\n",
    "model = Ridge()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(1, 0.01)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de310e6",
   "metadata": {},
   "source": [
    "Running the example will evaluate each combination of configurations using repeated cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
    "\n",
    "In this case, we can see that we achieved slightly better results than the default 3.379 vs. 3.382. Ignore the sign; the library makes the MAE negative for optimization purposes.\n",
    "\n",
    "We can see that the model assigned an alpha weight of 0.51 to the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e32454",
   "metadata": {},
   "source": [
    "The scikit-learn library also provides a built-in version of the algorithm that automatically finds good hyperparameters via the RidgeCV class.\n",
    "\n",
    "To use this class, it is fit on the training dataset and used to make a prediction. During the training process, it automatically tunes the hyperparameter values.\n",
    "\n",
    "By default, the model will only test the alpha values (0.1, 1.0, 10.0). We can change this to a grid of values between 0 and 1 with a separation of 0.01 as we did on the previous example by setting the “alphas” argument.\n",
    "\n",
    "The example below demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5268782f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "alphas[0] == 0.0, must be > 0.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43720/3849789197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# summarize chosen configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alpha: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_alphas\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m                     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"alphas[{index}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"alphas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_scalar\u001b[0;34m(x, name, target_type, min_val, max_val, include_boundaries)\u001b[0m\n\u001b[1;32m   1478\u001b[0m     )\n\u001b[1;32m   1479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomparison_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1481\u001b[0m             \u001b[0;34mf\"{name} == {x}, must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             \u001b[0;34mf\" {'>=' if include_boundaries in ('left', 'both') else '>'} {min_val}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: alphas[0] == 0.0, must be > 0.0."
     ]
    }
   ],
   "source": [
    "# use automatically configured the ridge regression algorithm\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "model = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdad31",
   "metadata": {},
   "source": [
    "Running the example fits the model and discovers the hyperparameters that give the best results using cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
    "\n",
    "In this case, we can see that the model chose the identical hyperparameter of alpha=0.51 that we found via our manual grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
